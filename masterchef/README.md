analytics-brand-tracker
=======================

Masterchef  cooks text from/to HDFS. Originally, pretended to be a  pipeline to process documents applying a set of configurable functions. Finally, due to
bureaucratic inconvenients it has been built as a collection of MR jobs and simple python helper scripts.


Installing and dependencies
------------
Dependencies for  masterchef are:
* NLTK libray: nltk-2.0.4
* PyYAML-3.10 (a dependency of NLTK)
* Feathers:  https://github.com/klbostee/feathers(needed to output to multiples directories from reduce functions)

These dependencies do not need to be installed on each node, they are uploaded as dependencies instead(and currently uploaded to the repository. See conf directory)


Usage info
------------
The main pipeline(a collectio of MR jobs) is executed by a bash script. Currently the MR jobs are executed independently(because some of the MR jobs are
Twitter dependent and they may want to be used to cook other kind of documents):

```
    cd $ANALITYCSTEXTMINING_HOME

    ./scripts/masterchef.sh OPTION(0|1|2) input_dir output_dir
```
Options:
0) PREPARING:'Twitter dependent(for now)': expects json as input files where 'id_str' is the tweet identifier and 'created_at' contantains
the date of the tweet. Tweets from INPUT_DIR are copied to OUTPUT_DIR/date=yyyy-MM-dd, removing duplicated and with parsed date.

1) COOKING: Cooks text. Expects one json per line (from INPUT_DIR) and applies configurable functions to remove stopwords, expressions, etc. (See cooking.py script and 'cooking' section in conf/masterchef.ini for details)

2) SERVING: Expects one json per line. Outputs configurable set of fields from the json docs.(See 'serving' section in conf/masterchef.ini for details).


Input_dir: from HDFS
Output_dir: to HDFS


Also, the miscellaneous directory contains helper scripts:
* officialusers.py: the purpose of this job is to be able to identify oficial users(not useful to brand tracker).
* parsercrm.py: takes as input a directory containing the CRM files provided by Per√∫ and generates as output a file with a document(conversation) per line 
* parserlemmatizer.py: takes as input the output generated by lemmatizer and outputs each document in one line
* parsertableau.py: takes as input a CVS file and generates the input file expected by Tableau
* tweetsbyconversation.py: groups tweets by conversation. A conversation is defined as a directed graph of tweets built throught
the field 'in_reply_to_status_id_str'. For any tweet different that the root, the field 'in_reply_to_status_id_str'
is not empty. There is only a Root has the field in_reply_to_status_id_str' empty. Cycles can exist. Input and output files format is expected to be json.
* parsertass.py: given an xml file: http://www.daedalus.es/TASS2013/data/general-tweets-train-tagged.xml, for each tweet, the original status is retrieved 
and an output file is generated mergingthe status information along with the Sentimental Analisis and Topic Detection tagging from TASS
